{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Al Karama Urban Climate Analysis - Code Guide\n",
    "\n",
    "This notebook demonstrates the Python code and libraries used for each analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Library Overview](#1-library-overview)\n",
    "2. [Street View Analysis (ZenSVI)](#2-street-view-analysis)\n",
    "3. [Satellite Analysis (Google Earth Engine)](#3-satellite-analysis)\n",
    "4. [Network Analysis (OSMnx + NetworkX)](#4-network-analysis)\n",
    "5. [Cluster Analysis (scikit-learn)](#5-cluster-analysis)\n",
    "6. [Visualization (Folium/Leaflet)](#6-visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Library Overview\n",
    "\n",
    "### Required Libraries by Analysis Type\n",
    "\n",
    "| Analysis | Libraries | Install Command |\n",
    "|----------|-----------|----------------|\n",
    "| Street View (GVI, SVF) | `zensvi` | `pip install zensvi` |\n",
    "| Satellite (LST, NDVI) | `earthengine-api` | `pip install earthengine-api` |\n",
    "| Network (Centrality) | `osmnx`, `networkx` | `pip install osmnx networkx` |\n",
    "| Clustering | `scikit-learn` | `pip install scikit-learn` |\n",
    "| Spatial Data | `geopandas`, `shapely` | `pip install geopandas shapely` |\n",
    "| Visualization | `folium`, `matplotlib` | `pip install folium matplotlib` |\n",
    "| Data Processing | `pandas`, `numpy`, `scipy` | `pip install pandas numpy scipy` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries (run once)\n",
    "# !pip install zensvi earthengine-api osmnx networkx scikit-learn geopandas folium pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installed versions\n",
    "import pkg_resources\n",
    "\n",
    "packages = ['zensvi', 'earthengine-api', 'osmnx', 'networkx', \n",
    "            'scikit-learn', 'geopandas', 'folium', 'pandas', 'numpy']\n",
    "\n",
    "print(\"Installed Package Versions:\")\n",
    "print(\"-\" * 40)\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(pkg).version\n",
    "        print(f\"{pkg:20s} {version}\")\n",
    "    except:\n",
    "        print(f\"{pkg:20s} NOT INSTALLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Street View Analysis\n",
    "\n",
    "### Libraries Required\n",
    "```python\n",
    "from zensvi.download import MLYDownloader  # Download Mapillary images\n",
    "from zensvi.cv import ClassifierGVI        # Calculate Green View Index\n",
    "from zensvi.cv import ClassifierSVF        # Calculate Sky View Factor\n",
    "```\n",
    "\n",
    "### 2.1 Download Street View Images from Mapillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zensvi.download import MLYDownloader\n",
    "\n",
    "# Initialize downloader\n",
    "mly = MLYDownloader(log_path=\"logs\")\n",
    "\n",
    "# Download images within a GeoJSON boundary\n",
    "mly.download_svi(\n",
    "    dir_output=\"data/mapillary_svi\",      # Output directory\n",
    "    input_shp=\"data/al_karama.geojson\",   # Study area boundary\n",
    "    lat=None,                              # Or specify lat/lon for point\n",
    "    lon=None,\n",
    "    id_columns=None\n",
    ")\n",
    "\n",
    "# Note: Requires Mapillary API token in environment variable MLY_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculate Green View Index (GVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zensvi.cv import ClassifierGVI\n",
    "\n",
    "# Initialize GVI classifier\n",
    "gvi_classifier = ClassifierGVI()\n",
    "\n",
    "# Calculate GVI for all images in a directory\n",
    "gvi_classifier.classify(\n",
    "    dir_input=\"data/mapillary_svi\",       # Input image directory\n",
    "    dir_output=\"output/gvi_results\",       # Output directory\n",
    "    csv_output=\"output/gvi_results.csv\"    # Results CSV\n",
    ")\n",
    "\n",
    "# GVI is calculated as:\n",
    "# GVI = (green pixels) / (total pixels) * 100%\n",
    "# Uses vegetation detection based on color analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculate Sky View Factor (SVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zensvi.cv import ClassifierSVF\n",
    "\n",
    "# Initialize SVF classifier  \n",
    "svf_classifier = ClassifierSVF()\n",
    "\n",
    "# Calculate SVF for all images\n",
    "svf_classifier.classify(\n",
    "    dir_input=\"data/mapillary_svi\",\n",
    "    dir_output=\"output/svf_results\",\n",
    "    csv_output=\"output/svf_results.csv\"\n",
    ")\n",
    "\n",
    "# SVF is calculated as:\n",
    "# SVF = (sky pixels) / (upper hemisphere pixels) * 100%\n",
    "# Lower SVF = more shade from buildings/trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Satellite Analysis\n",
    "\n",
    "### Libraries Required\n",
    "```python\n",
    "import ee  # Google Earth Engine\n",
    "```\n",
    "\n",
    "### 3.1 Initialize Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# First time: Authenticate (opens browser)\n",
    "# ee.Authenticate()\n",
    "\n",
    "# Initialize with your project\n",
    "ee.Initialize(project='your-project-id')  # Replace with your GEE project\n",
    "\n",
    "print(\"Google Earth Engine initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define Study Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box for Al Karama\n",
    "north, south = 25.255, 25.230\n",
    "east, west = 55.315, 55.290\n",
    "\n",
    "# Create Earth Engine geometry\n",
    "study_area = ee.Geometry.Rectangle([west, south, east, north])\n",
    "\n",
    "print(f\"Study area: {(north-south)*111:.1f}km x {(east-west)*111:.1f}km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculate Land Surface Temperature (LST) from Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lst(image):\n",
    "    \"\"\"\n",
    "    Calculate Land Surface Temperature from Landsat 8/9 thermal band.\n",
    "    \n",
    "    Formula:\n",
    "    LST = BT / (1 + (λ × BT / ρ) × ln(ε))\n",
    "    \n",
    "    Where:\n",
    "    - BT = Brightness Temperature\n",
    "    - λ = wavelength (10.8 μm for Landsat Band 10)\n",
    "    - ρ = h × c / σ = 14380 μm·K\n",
    "    - ε = emissivity (0.95 for urban)\n",
    "    \"\"\"\n",
    "    # Get thermal band and convert to brightness temperature\n",
    "    thermal = image.select('ST_B10').multiply(0.00341802).add(149.0)\n",
    "    \n",
    "    # Constants\n",
    "    wavelength = 10.8  # μm\n",
    "    rho = 14380        # μm·K\n",
    "    emissivity = 0.95  # urban surfaces\n",
    "    \n",
    "    # Calculate LST in Celsius\n",
    "    lst = thermal.divide(\n",
    "        ee.Number(1).add(\n",
    "            ee.Number(wavelength).multiply(thermal).divide(rho)\n",
    "            .multiply(ee.Number(emissivity).log())\n",
    "        )\n",
    "    ).subtract(273.15)  # Convert Kelvin to Celsius\n",
    "    \n",
    "    return image.addBands(lst.rename('LST'))\n",
    "\n",
    "# Load Landsat 8/9 Collection 2\n",
    "landsat = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2') \\\n",
    "    .filterBounds(study_area) \\\n",
    "    .filterDate('2024-06-01', '2024-08-31') \\\n",
    "    .filter(ee.Filter.lt('CLOUD_COVER', 20)) \\\n",
    "    .map(calculate_lst)\n",
    "\n",
    "# Get median LST\n",
    "lst_image = landsat.select('LST').median().clip(study_area)\n",
    "\n",
    "print(f\"Landsat images found: {landsat.size().getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculate NDVI from Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indices(image):\n",
    "    \"\"\"\n",
    "    Calculate vegetation and built-up indices from Sentinel-2.\n",
    "    \n",
    "    NDVI = (NIR - Red) / (NIR + Red)\n",
    "    NDBI = (SWIR - NIR) / (SWIR + NIR)\n",
    "    \"\"\"\n",
    "    # NDVI: Normalized Difference Vegetation Index\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    \n",
    "    # NDBI: Normalized Difference Built-up Index\n",
    "    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
    "    \n",
    "    # NDWI: Normalized Difference Water Index\n",
    "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "    \n",
    "    return image.addBands([ndvi, ndbi, ndwi])\n",
    "\n",
    "# Load Sentinel-2 Surface Reflectance\n",
    "sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "    .filterBounds(study_area) \\\n",
    "    .filterDate('2024-01-01', '2024-12-31') \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "    .map(calculate_indices)\n",
    "\n",
    "# Get median indices\n",
    "indices = sentinel2.select(['NDVI', 'NDBI', 'NDWI']).median().clip(study_area)\n",
    "\n",
    "print(f\"Sentinel-2 images found: {sentinel2.size().getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Sample Data at Grid Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sample_satellite_data(combined_image, study_area, grid_spacing=0.0003):\n",
    "    \"\"\"\n",
    "    Sample satellite data at regular grid points.\n",
    "    \n",
    "    Args:\n",
    "        combined_image: ee.Image with bands to sample\n",
    "        study_area: ee.Geometry boundary\n",
    "        grid_spacing: degrees (~30m at equator)\n",
    "    \"\"\"\n",
    "    # Create grid points\n",
    "    bounds = study_area.bounds().getInfo()['coordinates'][0]\n",
    "    min_lon, min_lat = bounds[0]\n",
    "    max_lon, max_lat = bounds[2]\n",
    "    \n",
    "    lats = np.arange(min_lat, max_lat, grid_spacing)\n",
    "    lons = np.arange(min_lon, max_lon, grid_spacing)\n",
    "    \n",
    "    # Create feature collection of points\n",
    "    points = []\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            points.append(ee.Feature(ee.Geometry.Point([lon, lat])))\n",
    "    \n",
    "    fc = ee.FeatureCollection(points)\n",
    "    \n",
    "    # Sample the image\n",
    "    sampled = combined_image.sampleRegions(\n",
    "        collection=fc,\n",
    "        scale=10,  # 10m resolution\n",
    "        geometries=True\n",
    "    )\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "# Combine LST and indices into single image\n",
    "combined = lst_image.addBands(indices)\n",
    "\n",
    "# Sample (for small areas - for large areas, use batch processing)\n",
    "# sampled_data = sample_satellite_data(combined, study_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Network Analysis\n",
    "\n",
    "### Libraries Required\n",
    "```python\n",
    "import osmnx as ox      # Download OpenStreetMap street networks\n",
    "import networkx as nx   # Graph analysis\n",
    "```\n",
    "\n",
    "### 4.1 Download Street Network from OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "\n",
    "# Define bounding box\n",
    "north, south = 25.255, 25.230\n",
    "east, west = 55.315, 55.290\n",
    "\n",
    "# Download walking network\n",
    "G = ox.graph_from_bbox(\n",
    "    bbox=(north, south, east, west),\n",
    "    network_type='walk'  # Options: 'walk', 'drive', 'bike', 'all'\n",
    ")\n",
    "\n",
    "print(f\"Nodes (intersections): {G.number_of_nodes()}\")\n",
    "print(f\"Edges (street segments): {G.number_of_edges()}\")\n",
    "\n",
    "# Basic stats\n",
    "stats = ox.stats.basic_stats(G)\n",
    "print(f\"Total street length: {stats['street_length_total']/1000:.1f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to undirected for centrality analysis\n",
    "G_undir = G.to_undirected()\n",
    "\n",
    "# Calculate node betweenness centrality\n",
    "# This measures how often a node lies on shortest paths between other nodes\n",
    "node_betweenness = nx.betweenness_centrality(\n",
    "    G_undir, \n",
    "    weight='length',    # Use street length as weight\n",
    "    normalized=True     # Normalize to 0-1\n",
    ")\n",
    "\n",
    "# Add to graph as node attribute\n",
    "nx.set_node_attributes(G_undir, node_betweenness, 'betweenness')\n",
    "\n",
    "# Find top 5 most central nodes\n",
    "top_nodes = sorted(node_betweenness.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 nodes by betweenness centrality:\")\n",
    "for node_id, bc in top_nodes:\n",
    "    print(f\"  Node {node_id}: {bc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calculate Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closeness centrality\n",
    "# This measures how close a node is to all other nodes\n",
    "node_closeness = nx.closeness_centrality(\n",
    "    G_undir,\n",
    "    distance='length'  # Use street length as distance\n",
    ")\n",
    "\n",
    "# Add to graph\n",
    "nx.set_node_attributes(G_undir, node_closeness, 'closeness')\n",
    "\n",
    "print(f\"Closeness centrality - Mean: {np.mean(list(node_closeness.values())):.6f}\")\n",
    "print(f\"Closeness centrality - Max: {max(node_closeness.values()):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Convert to GeoDataFrame for Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert graph to GeoDataFrames\n",
    "nodes_gdf, edges_gdf = ox.graph_to_gdfs(G_undir)\n",
    "\n",
    "print(f\"Nodes GeoDataFrame: {len(nodes_gdf)} rows\")\n",
    "print(f\"Edges GeoDataFrame: {len(edges_gdf)} rows\")\n",
    "\n",
    "# Nodes now have betweenness and closeness as columns\n",
    "print(\"\\nNode columns:\", nodes_gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cluster Analysis\n",
    "\n",
    "### Libraries Required\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "### 5.1 Prepare Data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load your combined data\n",
    "df = pd.read_csv('output/walking_routes/point_comfort.csv')\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['gvi', 'svf', 'lst', 'ndvi']\n",
    "X = df[feature_cols].dropna()\n",
    "\n",
    "print(f\"Data points for clustering: {len(X)}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for K-means!)\n",
    "# This converts all features to z-scores (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  GVI range: {X['gvi'].min():.3f} - {X['gvi'].max():.3f}\")\n",
    "print(f\"  LST range: {X['lst'].min():.1f} - {X['lst'].max():.1f}\")\n",
    "\n",
    "print(\"\\nAfter scaling (all features now comparable):\")\n",
    "print(f\"  All features: mean ≈ 0, std ≈ 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Find Optimal Number of Clusters (Elbow Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal k\n",
    "inertias = []\n",
    "K_range = range(2, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (within-cluster sum of squares)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look for the 'elbow' where the curve bends - that's the optimal k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Run K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means with chosen k\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10  # Number of initializations\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "X['cluster'] = clusters\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"Cluster characteristics:\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(n_clusters):\n",
    "    cluster_data = X[X['cluster'] == i]\n",
    "    print(f\"\\nCluster {i} ({len(cluster_data)} points):\")\n",
    "    print(f\"  GVI:  {cluster_data['gvi'].mean()*100:.1f}%\")\n",
    "    print(f\"  SVF:  {cluster_data['svf'].mean()*100:.1f}%\")\n",
    "    print(f\"  LST:  {cluster_data['lst'].mean():.1f}°C\")\n",
    "    print(f\"  NDVI: {cluster_data['ndvi'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization\n",
    "\n",
    "### Libraries Required\n",
    "```python\n",
    "import folium           # Interactive maps\n",
    "import matplotlib.pyplot as plt  # Static plots\n",
    "```\n",
    "\n",
    "### 6.1 Create Interactive Map with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create base map centered on Al Karama\n",
    "m = folium.Map(\n",
    "    location=[25.2425, 55.3025],  # Center coordinates\n",
    "    zoom_start=15,\n",
    "    tiles='OpenStreetMap'\n",
    ")\n",
    "\n",
    "# Add circle markers for each point\n",
    "for idx, row in df.head(100).iterrows():  # Limit for demo\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fillOpacity=0.7,\n",
    "        popup=f\"GVI: {row['gvi']*100:.1f}%<br>LST: {row['lst']:.1f}°C\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display map (in Jupyter)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Color Points by Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(value, min_val, max_val, colormap='RdYlGn'):\n",
    "    \"\"\"\n",
    "    Convert a value to a color based on a colormap.\n",
    "    \"\"\"\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.colors as colors\n",
    "    \n",
    "    # Normalize value to 0-1\n",
    "    norm = (value - min_val) / (max_val - min_val)\n",
    "    norm = max(0, min(1, norm))  # Clamp to 0-1\n",
    "    \n",
    "    # Get color from colormap\n",
    "    cmap = cm.get_cmap(colormap)\n",
    "    rgba = cmap(norm)\n",
    "    \n",
    "    # Convert to hex\n",
    "    return colors.rgb2hex(rgba)\n",
    "\n",
    "# Create map with colored points by LST\n",
    "m2 = folium.Map(location=[25.2425, 55.3025], zoom_start=15)\n",
    "\n",
    "lst_min, lst_max = df['lst'].min(), df['lst'].max()\n",
    "\n",
    "for idx, row in df.head(500).iterrows():\n",
    "    color = get_color(row['lst'], lst_min, lst_max, 'RdYlGn_r')  # Red=hot\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=4,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fillColor=color,\n",
    "        fillOpacity=0.7\n",
    "    ).add_to(m2)\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_cols = ['gvi', 'svf', 'lst', 'ndvi', 'ndbi']\n",
    "corr_matrix = df[corr_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,           # Show values\n",
    "    cmap='RdBu_r',        # Red-Blue colormap\n",
    "    center=0,             # Center at 0\n",
    "    vmin=-1, vmax=1,      # Range -1 to 1\n",
    "    square=True,\n",
    "    fmt='.2f'             # 2 decimal places\n",
    ")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Putting It All Together\n",
    "\n",
    "### Complete Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# COMPLETE WORKFLOW SUMMARY\n",
    "# =============================================================\n",
    "\n",
    "# Step 1: Download street view images\n",
    "# from zensvi.download import MLYDownloader\n",
    "# mly = MLYDownloader()\n",
    "# mly.download_svi(dir_output=\"data/svi\", input_shp=\"boundary.geojson\")\n",
    "\n",
    "# Step 2: Calculate GVI and SVF\n",
    "# from zensvi.cv import ClassifierGVI, ClassifierSVF\n",
    "# ClassifierGVI().classify(dir_input=\"data/svi\", csv_output=\"gvi.csv\")\n",
    "# ClassifierSVF().classify(dir_input=\"data/svi\", csv_output=\"svf.csv\")\n",
    "\n",
    "# Step 3: Get satellite data from Google Earth Engine\n",
    "# import ee\n",
    "# ee.Initialize(project='your-project')\n",
    "# landsat = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')...\n",
    "# sentinel = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')...\n",
    "\n",
    "# Step 4: Download street network\n",
    "# import osmnx as ox\n",
    "# G = ox.graph_from_bbox(bbox=(...), network_type='walk')\n",
    "\n",
    "# Step 5: Calculate centrality\n",
    "# import networkx as nx\n",
    "# betweenness = nx.betweenness_centrality(G, weight='length')\n",
    "# closeness = nx.closeness_centrality(G, distance='length')\n",
    "\n",
    "# Step 6: Combine data and analyze\n",
    "# import pandas as pd\n",
    "# df = pd.merge(gvi_df, svf_df, on='id')\n",
    "# df = spatial_join(df, satellite_df)  # Join by location\n",
    "\n",
    "# Step 7: Calculate priority scores\n",
    "# df['priority'] = 0.4 * df['lst_norm'] + 0.25 * (1 - df['gvi_norm']) + ...\n",
    "\n",
    "# Step 8: Cluster analysis\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=4)\n",
    "# df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 9: Visualize\n",
    "# import folium\n",
    "# m = folium.Map(location=[lat, lon])\n",
    "# ...\n",
    "\n",
    "print(\"See the individual sections above for detailed code examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "### Libraries Documentation\n",
    "- ZenSVI: https://github.com/koito19960406/ZenSVI\n",
    "- Google Earth Engine: https://developers.google.com/earth-engine\n",
    "- OSMnx: https://osmnx.readthedocs.io/\n",
    "- NetworkX: https://networkx.org/documentation/\n",
    "- scikit-learn: https://scikit-learn.org/stable/\n",
    "- Folium: https://python-visualization.github.io/folium/\n",
    "\n",
    "### Academic References\n",
    "- GVI: Yang et al. (2009) \"The urban forest in Beijing and its role in air pollution reduction\"\n",
    "- SVF: Oke (1981) \"Canyon geometry and the nocturnal urban heat island\"\n",
    "- NDVI: Tucker (1979) \"Red and photographic infrared linear combinations for monitoring vegetation\"\n",
    "- Betweenness Centrality: Freeman (1977) \"A set of measures of centrality based on betweenness\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
